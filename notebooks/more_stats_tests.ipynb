{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up representative dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process BAM Files: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n",
      "Process methylation: 100%|██████████| 1/1 [00:04<00:00,  4.46s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "from pyllelic import pyllelic\n",
    "\n",
    "config = pyllelic.configure(\n",
    "    base_path=\"../assets/\",\n",
    "    prom_file=\"tert_genome.txt\",\n",
    "    prom_start=1293200,\n",
    "    prom_end=1296000,\n",
    "    chrom=\"5\",\n",
    "    offset=1293000,\n",
    ")\n",
    "\n",
    "files_set = pyllelic.make_list_of_bam_files(config)\n",
    "\n",
    "data = pyllelic.pyllelic(config=config, files_set=files_set)\n",
    "\n",
    "df = data.individual_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = {  # Pulled from real dataset\n",
    "    \"allelic\": df[\"1295771\"].tolist()[0],\n",
    "    \"non-allelic\": df[\"1293589\"].tolist()[0],\n",
    "    \"question\": df[\"1294420\"].tolist()[0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "for val in tests.values():\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define test approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffs(data_list):\n",
    "    \"\"\"Find the difference between list mean and mode, \n",
    "    assuming larger is  more allelic\"\"\"\n",
    "    \n",
    "    means = float(np.mean(data_list))\n",
    "    modes = stats.mode(data_list)[0][0]\n",
    "    return abs(means-modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_stats(data_list):\n",
    "    \"\"\"Calculate Anderson-Darling stat, assuming normal distribution.\"\"\"\n",
    "    stat, crits, _ = stats.anderson(data_list)\n",
    "    is_sig = bool(stat > crits[4])  # type: ignore\n",
    "    return (is_sig, stat, crits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Module for computing The Hartigans' dip statistic\n",
    "    The dip statistic measures unimodality of a sample from a random process.\n",
    "    See: \n",
    "    Hartigan, J. A.; Hartigan, P. M. The Dip Test of Unimodality. The Annals \n",
    "    of Statistics 13 (1985), no. 1, 70--84. doi:10.1214/aos/1176346577. \n",
    "    http://projecteuclid.org/euclid.aos/1176346577.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def _gcm_(cdf, idxs):\n",
    "    work_cdf = cdf\n",
    "    work_idxs = idxs\n",
    "    gcm = [work_cdf[0]]\n",
    "    touchpoints = [0]\n",
    "    while len(work_cdf) > 1:\n",
    "        distances = work_idxs[1:] - work_idxs[0]\n",
    "        slopes = (work_cdf[1:] - work_cdf[0]) / distances\n",
    "        minslope = slopes.min()\n",
    "        minslope_idx = np.where(slopes == minslope)[0][0] + 1\n",
    "        gcm.extend(work_cdf[0] + distances[:minslope_idx] * minslope)\n",
    "        touchpoints.append(touchpoints[-1] + minslope_idx)\n",
    "        work_cdf = work_cdf[minslope_idx:]\n",
    "        work_idxs = work_idxs[minslope_idx:]\n",
    "    return np.array(np.array(gcm)),np.array(touchpoints)\n",
    "\n",
    "def _lcm_(cdf, idxs):\n",
    "    g,t = _gcm_(1-cdf[::-1], idxs.max() - idxs[::-1])\n",
    "    return 1-g[::-1], len(cdf) - 1 - t[::-1]\n",
    "\n",
    "def _touch_diffs_(part1, part2, touchpoints):\n",
    "    diff = np.abs((part2[touchpoints] - part1[touchpoints]))\n",
    "    return diff.max(), diff\n",
    "\n",
    "def dip(histogram=None, idxs=None):\n",
    "    \"\"\"\n",
    "        Compute the Hartigans' dip statistic either for a histogram of\n",
    "        samples (with equidistant bins) or for a set of samples.\n",
    "    \"\"\"\n",
    "    if idxs is None:\n",
    "        idxs = np.arange(len(histogram))\n",
    "    elif histogram is None:\n",
    "        h = collections.Counter(idxs)\n",
    "        idxs = np.msort(h.keys())\n",
    "        histogram = np.array([h[i] for i in idxs])\n",
    "    else:\n",
    "        if len(histogram) != len(idxs):\n",
    "            raise ValueError(\"Need exactly as many indices as histogram bins.\")\n",
    "        if len(idxs) != len(set(idxs)):\n",
    "            raise ValueError(\"idxs must be unique if histogram is given.\")\n",
    "        if not np.array_equal(np.msort(idxs), idxs):\n",
    "            idxs_s = np.argsort(idxs)\n",
    "            idx = np.asarray(idxs)[idxs_s]\n",
    "            histogram = np.asarray(histogram)[idxs_s]\n",
    "\n",
    "    cdf = np.cumsum(histogram, dtype=float)\n",
    "    cdf /= cdf[-1]\n",
    "\n",
    "    work_idxs = idxs\n",
    "    work_histogram = np.asarray(histogram, dtype=float) / np.sum(histogram)\n",
    "    work_cdf = cdf\n",
    "\n",
    "    D = 0\n",
    "    left = [0]\n",
    "    right = [1]\n",
    "\n",
    "    while True:\n",
    "        left_part, left_touchpoints   = _gcm_(work_cdf - work_histogram, work_idxs)\n",
    "        right_part, right_touchpoints = _lcm_(work_cdf, work_idxs)\n",
    "\n",
    "        d_left, left_diffs   = _touch_diffs_(left_part, right_part, left_touchpoints)\n",
    "        d_right, right_diffs = _touch_diffs_(left_part, right_part, right_touchpoints)\n",
    "\n",
    "        if d_right > d_left:\n",
    "            xr = right_touchpoints[d_right == right_diffs][-1]\n",
    "            xl = left_touchpoints[left_touchpoints <= xr][-1]\n",
    "            d  = d_right\n",
    "        else:\n",
    "            xl = left_touchpoints[d_left == left_diffs][0]\n",
    "            xr = right_touchpoints[right_touchpoints >= xl][0]\n",
    "            d  = d_left\n",
    "\n",
    "        left_diff  = np.abs(left_part[:xl+1] - work_cdf[:xl+1]).max()\n",
    "        right_diff = np.abs(right_part[xr:]  - work_cdf[xr:] + work_histogram[xr:]).max()\n",
    "\n",
    "        if d <= D or xr == 0 or xl == len(work_cdf):\n",
    "            the_dip = max(np.abs(cdf[:len(left)] - left).max(), np.abs(cdf[-len(right)-1:-1] - right).max())\n",
    "            return the_dip/2, (cdf, idxs, left, left_part, right, right_part)\n",
    "        else:\n",
    "            D = max(D, left_diff, right_diff)\n",
    "\n",
    "        work_cdf = work_cdf[xl:xr+1]\n",
    "        work_idxs = work_idxs[xl:xr+1]\n",
    "        work_histogram = work_histogram[xl:xr+1]\n",
    "\n",
    "        left[len(left):] = left_part[1:xl+1]\n",
    "        right[:0] = right_part[xr:-1]\n",
    "\n",
    "def plot_dip(histogram=None, idxs=None):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    d,(cdf,idxs,left,left_part,right,right_part) = dip(histogram,idxs)\n",
    "\n",
    "\n",
    "    plt.plot(idxs[:len(left)], left, color='red')\n",
    "    plt.plot(idxs[len(left)-1:len(left)+len(left_part) - 1], left_part, color='gray')\n",
    "    plt.plot(idxs[-len(right):], right, color='blue')\n",
    "    plt.plot(idxs[len(cdf) - len(right) + 1 - len(right_part):len(cdf) - len(right) + 1], right_part, color='gray')\n",
    "\n",
    "    the_dip = max(np.abs(cdf[:len(left)] - left).max(), np.abs(cdf[-len(right)-1:-1] - right).max())\n",
    "    l_dip_idxs = np.abs(cdf[:len(left)] - left) == the_dip\n",
    "    r_dip_idxs = np.abs(cdf[-len(right)-1:-1] - right) == the_dip\n",
    "    print(the_dip/2, d)\n",
    "\n",
    "    plt.vlines(x=idxs[:len(left)][l_dip_idxs], ymin=cdf[:len(left)][l_dip_idxs], ymax = cdf[:len(left)][l_dip_idxs] - the_dip)\n",
    "    plt.vlines(x=idxs[-len(right):][r_dip_idxs], ymin=cdf[-len(right)-1:-1][r_dip_idxs], ymax = cdf[-len(right)-1:][r_dip_idxs] + the_dip)\n",
    "\n",
    "    plt.plot(np.repeat(idxs,2)[1:], np.repeat(cdf,2)[:-1], color='black')\n",
    "    plt.scatter(idxs, cdf)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def crit_points(random_function, quantiles, sample_size, n_samples):\n",
    "    \"\"\"\n",
    "        Compute the quantiles for the dip statistic for n_samples \n",
    "        samples of size sample_size from the random process given by \n",
    "        random_function.\n",
    "        Parameters:\n",
    "        random_function : a paramter-free function which returns random values.\n",
    "        quantiles : a sequence of values between 0 and 1\n",
    "        sample_size : the size of the samples to draw from random_function\n",
    "        n_samples : the number of samples for which to compute dips\n",
    "        Returns: a list such that the i'th value is the greatest dip observed\n",
    "        such that the fraction of dips less than or equal to that value is less\n",
    "        than the i'th value from quantiles.\n",
    "    \"\"\"\n",
    "    data = [[random_function() for _ in range(sample_size)] for __ in range(n_samples)]\n",
    "    dips = np.array([dip(idxs=samples)[0] for samples in data])\n",
    "    \n",
    "    return np.percentile(dips, [p * 100 for p in quantiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hartigan_dip(data_list):\n",
    "   \"\"\"Calculate Hartigan dip test statistic\"\"\"\n",
    "   return dip(data_list)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture(data_list):\n",
    "    train_data = np.array([[100,  0,  0,  0, 100],\n",
    "       [100,  0,  0,  0, 100]])\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    GMM = GaussianMixture(n_components=2, random_state=0).fit(train_data)\n",
    "    data_hist = np.histogram(data_list, bins=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    data_array = np.array([data_hist[0], data_hist[0]])\n",
    "   #  print(data_array)\n",
    "    \n",
    "    return round(GMM.score(data_array),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {  # Save our methods in a dictionary\n",
    "    \"diffs\": diffs, \n",
    "    \"ad_stats\": ad_stats,\n",
    "    \"dip_test\": hartigan_dip,\n",
    "    \"gaussian_mixture\": gaussian_mixture,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test all the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "testing diffs\n",
      "--------------------\n",
      "expected: allelic, result: 0.5\n",
      "expected: non-allelic, result: 0.0\n",
      "expected: question, result: 0.30434782608695654\n",
      "====================\n",
      "testing ad_stats\n",
      "--------------------\n",
      "expected: allelic, result: (True, 8.099645009495575, array([0.536, 0.61 , 0.732, 0.854, 1.016]))\n",
      "expected: non-allelic, result: (False, nan, array([0.543, 0.618, 0.742, 0.865, 1.029]))\n",
      "expected: question, result: (True, 4.8279084069810345, array([0.511, 0.582, 0.699, 0.815, 0.969]))\n",
      "====================\n",
      "testing dip_test\n",
      "--------------------\n",
      "expected: allelic, result: 0.02173913043478265\n",
      "expected: non-allelic, result: 0.07142857142857145\n",
      "expected: question, result: 0.07142857142857145\n",
      "====================\n",
      "testing gaussian_mixture\n",
      "--------------------\n",
      "expected: allelic, result: -529000004.49\n",
      "expected: non-allelic, result: -24500004.49\n",
      "expected: question, result: -152500004.49\n"
     ]
    }
   ],
   "source": [
    "for name, method in methods.items():\n",
    "    print(\"=\"*20)\n",
    "    print(f\"testing {name}\")\n",
    "    print(\"-\"*20)\n",
    "    for k,v in tests.items():\n",
    "        print(f\"expected: {k}, result: {method(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e78778451c7b2396497e141b211211f41052d8ea57a416e99a69fb068340a3a3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('allelic': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
